#include "kernel.h"
#include <immintrin.h>

/*
 * 24×6 micro‑kernel (AVX‑512, FMA, fully unrolled & **fixed strides**)
 *
 *  Packed‑A layout: for each k‑slice, 24 contiguous doubles (row 0‑23).
 *  Packed‑B layout: for each k‑slice,               6 doubles (col 0‑5).
 *
 *  The pointer bumps inside the kc loop must therefore be **constant**
 *  (MR=24, NR=6) regardless of tail sizes; using variable `mr`/`nr`
 *  corrupts the packed‑panel alignment.  This version corrects that.
 */

#define MR 24
#define NR  6

void kernel_24x6(const double *restrict blockA_packed,
                 const double *restrict blockB_packed,
                 double *restrict       C,
                 int                    mr,
                 int                    nr,
                 int                    kc,
                 int                    m)
{
    /* build masks for partial‑row tails */
    const __mmask8 FULL = 0xFF;
    __mmask8 mask0 = (mr >=  8) ? FULL : (__mmask8)((1u << mr) - 1u);
    __mmask8 mask1 = (mr <=  8) ? 0    : (mr >= 16 ? FULL : (__mmask8)((1u << (mr -  8)) - 1u));
    __mmask8 mask2 = (mr <= 16) ? 0    : (mr >= 24 ? FULL : (__mmask8)((1u << (mr - 16)) - 1u));

    /* 18 accumulators: 3 row groups × up to 6 cols */
    __m512d C00,C01,C02, C10,C11,C12, C20,C21,C22;
    __m512d C30,C31,C32, C40,C41,C42, C50,C51,C52;

#define LOAD_COL(col, dst0, dst1, dst2)                                    \
    dst0 = _mm512_maskz_loadu_pd(mask0, C + (size_t)(col) * m);            \
    dst1 = _mm512_maskz_loadu_pd(mask1, C + (size_t)(col) * m + 8);        \
    dst2 = _mm512_maskz_loadu_pd(mask2, C + (size_t)(col) * m + 16);

    if (nr >= 1) LOAD_COL(0, C00,C01,C02);
    if (nr >= 2) LOAD_COL(1, C10,C11,C12);
    if (nr >= 3) LOAD_COL(2, C20,C21,C22);
    if (nr >= 4) LOAD_COL(3, C30,C31,C32);
    if (nr >= 5) LOAD_COL(4, C40,C41,C42);
    if (nr >= 6) LOAD_COL(5, C50,C51,C52);
#undef LOAD_COL

    const double *a_ptr = blockA_packed;
    const double *b_ptr = blockB_packed;

    for (int p = 0; p < kc; ++p) {
        __m512d a0 = _mm512_loadu_pd(a_ptr + 0);
        __m512d a1 = _mm512_loadu_pd(a_ptr + 8);
        __m512d a2 = _mm512_loadu_pd(a_ptr + 16);

#define FMADD_COL(col, c0,c1,c2)                                           \
        if (nr > (col)) {                                                  \
            __m512d b = _mm512_set1_pd(b_ptr[col]);                        \
            c0 = _mm512_fmadd_pd(a0, b, c0);                               \
            c1 = _mm512_fmadd_pd(a1, b, c1);                               \
            c2 = _mm512_fmadd_pd(a2, b, c2);                               \
        }
        FMADD_COL(0, C00,C01,C02);
        FMADD_COL(1, C10,C11,C12);
        FMADD_COL(2, C20,C21,C22);
        FMADD_COL(3, C30,C31,C32);
        FMADD_COL(4, C40,C41,C42);
        FMADD_COL(5, C50,C51,C52);
#undef FMADD_COL

        a_ptr += MR;  /* *** fixed stride: 24 doubles *** */
        b_ptr += NR;  /* *** fixed stride:  6 doubles *** */
    }

#define STORE_COL(col, src0,src1,src2)                                     \
    _mm512_mask_storeu_pd(C + (size_t)(col) * m,      mask0, src0);        \
    if (mask1) _mm512_mask_storeu_pd(C + (size_t)(col) * m + 8,  mask1, src1); \
    if (mask2) _mm512_mask_storeu_pd(C + (size_t)(col) * m + 16, mask2, src2);

    if (nr >= 1) STORE_COL(0, C00,C01,C02);
    if (nr >= 2) STORE_COL(1, C10,C11,C12);
    if (nr >= 3) STORE_COL(2, C20,C21,C22);
    if (nr >= 4) STORE_COL(3, C30,C31,C32);
    if (nr >= 5) STORE_COL(4, C40,C41,C42);
    if (nr >= 6) STORE_COL(5, C50,C51,C52);
#undef STORE_COL
}
