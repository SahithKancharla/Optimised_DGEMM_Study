// kernel_24x6.c
#include "kernel.h"
#include <immintrin.h>

#define MR 24
#define NR  6

// Build a mask of up to 8 bits starting at `offset`, for a total of `rows` valid lanes.
static inline __mmask8 row_mask(int rows, int offset) {
    int r = rows - offset;
    if (r <= 0) return 0;
    if (r >= 8) return 0xFF;
    return (1u << r) - 1u;
}

void kernel_24x6(
    const double *restrict A,
    const double *restrict B,
    double       *restrict C,
    int                    mr,
    int                    nr,
    int                    kc,
    int                    ldC
) {
    // Fast‑path: full 24 rows, no masking needed
    if (mr == MR) {
        // 1) load C into registers
        __m512d C0[3], C1[3], C2[3], C3[3], C4[3], C5[3];
        #define LOAD_C(col,Cv)                          \
          Cv[0] = _mm512_load_pd(C + (col)*ldC +  0);   \
          Cv[1] = _mm512_load_pd(C + (col)*ldC +  8);   \
          Cv[2] = _mm512_load_pd(C + (col)*ldC + 16)
        switch(nr) {
          default: LOAD_C(5, C5);
          case 5:  LOAD_C(4, C4);
          case 4:  LOAD_C(3, C3);
          case 3:  LOAD_C(2, C2);
          case 2:  LOAD_C(1, C1);
          case 1:  LOAD_C(0, C0);
          case 0: ;  // nothing
        }
        #undef LOAD_C

        // 2) inner kc loop
        for (int p = 0; p + 1 < kc; p += 2) {
            // load A:
            __m512d a0 = _mm512_load_pd(A +  0);
            __m512d a1 = _mm512_load_pd(A +  8);
            __m512d a2 = _mm512_load_pd(A + 16);

            // for each column < nr, broadcast B[col] and FMA
            // unconditionally computing C0..C5 but only up to nr
            if (nr > 0) {
                __m512d b0 = _mm512_set1_pd(B[0]);
                C0[0] = _mm512_fmadd_pd(a0, b0, C0[0]);
                C0[1] = _mm512_fmadd_pd(a1, b0, C0[1]);
                C0[2] = _mm512_fmadd_pd(a2, b0, C0[2]);
            }
            if (nr > 1) {
                __m512d b1 = _mm512_set1_pd(B[1]);
                C1[0] = _mm512_fmadd_pd(a0, b1, C1[0]);
                C1[1] = _mm512_fmadd_pd(a1, b1, C1[1]);
                C1[2] = _mm512_fmadd_pd(a2, b1, C1[2]);
            }
            if (nr > 2) {
                __m512d b2 = _mm512_set1_pd(B[2]);
                C2[0] = _mm512_fmadd_pd(a0, b2, C2[0]);
                C2[1] = _mm512_fmadd_pd(a1, b2, C2[1]);
                C2[2] = _mm512_fmadd_pd(a2, b2, C2[2]);
            }
            if (nr > 3) {
                __m512d b3 = _mm512_set1_pd(B[3]);
                C3[0] = _mm512_fmadd_pd(a0, b3, C3[0]);
                C3[1] = _mm512_fmadd_pd(a1, b3, C3[1]);
                C3[2] = _mm512_fmadd_pd(a2, b3, C3[2]);
            }
            if (nr > 4) {
                __m512d b4 = _mm512_set1_pd(B[4]);
                C4[0] = _mm512_fmadd_pd(a0, b4, C4[0]);
                C4[1] = _mm512_fmadd_pd(a1, b4, C4[1]);
                C4[2] = _mm512_fmadd_pd(a2, b4, C4[2]);
            }
            if (nr > 5) {
                __m512d b5 = _mm512_set1_pd(B[5]);
                C5[0] = _mm512_fmadd_pd(a0, b5, C5[0]);
                C5[1] = _mm512_fmadd_pd(a1, b5, C5[1]);
                C5[2] = _mm512_fmadd_pd(a2, b5, C5[2]);
            }

            A += MR;  // move to next A panel row
            B += NR;  // move to next B column
        }

        // 3) store C back
        #define STORE_C(col,Cv)                          \
          _mm512_store_pd(C + (col)*ldC +  0, Cv[0]);    \
          _mm512_store_pd(C + (col)*ldC +  8, Cv[1]);    \
          _mm512_store_pd(C + (col)*ldC + 16, Cv[2])
        switch(nr) {
          default: STORE_C(5, C5);
          case 5:  STORE_C(4, C4);
          case 4:  STORE_C(3, C3);
          case 3:  STORE_C(2, C2);
          case 2:  STORE_C(1, C1);
          case 1:  STORE_C(0, C0);
          case 0: ;
        }
        #undef STORE_C

        return;
    }

    // —– slow path: masked loads/stores for mr < 24 —–
    const __mmask8 m0 = row_mask(mr,   0);
    const __mmask8 m1 = row_mask(mr,   8);
    const __mmask8 m2 = row_mask(mr,  16);

    // 1) load C masked
    __m512d C0[3], C1[3], C2[3], C3[3], C4[3], C5[3];
    #define LOAD_C_M(col,Cv)                                     \
      Cv[0] = _mm512_maskz_loadu_pd(m0, C + (col)*ldC +  0);      \
      Cv[1] = _mm512_maskz_loadu_pd(m1, C + (col)*ldC +  8);      \
      Cv[2] = _mm512_maskz_loadu_pd(m2, C + (col)*ldC + 16)
    switch(nr) {
      default: LOAD_C_M(5, C5);
      case 5:  LOAD_C_M(4, C4);
      case 4:  LOAD_C_M(3, C3);
      case 3:  LOAD_C_M(2, C2);
      case 2:  LOAD_C_M(1, C1);
      case 1:  LOAD_C_M(0, C0);
      case 0: ;
    }
    #undef LOAD_C_M

    // 2) same braodcast+FMA loop
    for (int p = 0; p < kc; ++p) {
        __m512d a0 = _mm512_maskz_loadu_pd(m0, A +  0);
        __m512d a1 = _mm512_maskz_loadu_pd(m1, A +  8);
        __m512d a2 = _mm512_maskz_loadu_pd(m2, A + 16);

        if (nr > 0) {
            __m512d b0 = _mm512_set1_pd(B[0]);
            C0[0] = _mm512_fmadd_pd(a0, b0, C0[0]);
            C0[1] = _mm512_fmadd_pd(a1, b0, C0[1]);
            C0[2] = _mm512_fmadd_pd(a2, b0, C0[2]);
        }
        if (nr > 1) {
            __m512d b1 = _mm512_set1_pd(B[1]);
            C1[0] = _mm512_fmadd_pd(a0, b1, C1[0]);
            C1[1] = _mm512_fmadd_pd(a1, b1, C1[1]);
            C1[2] = _mm512_fmadd_pd(a2, b1, C1[2]);
        }
        if (nr > 2) {
            __m512d b2 = _mm512_set1_pd(B[2]);
            C2[0] = _mm512_fmadd_pd(a0, b2, C2[0]);
            C2[1] = _mm512_fmadd_pd(a1, b2, C2[1]);
            C2[2] = _mm512_fmadd_pd(a2, b2, C2[2]);
        }
        if (nr > 3) {
            __m512d b3 = _mm512_set1_pd(B[3]);
            C3[0] = _mm512_fmadd_pd(a0, b3, C3[0]);
            C3[1] = _mm512_fmadd_pd(a1, b3, C3[1]);
            C3[2] = _mm512_fmadd_pd(a2, b3, C3[2]);
        }
        if (nr > 4) {
            __m512d b4 = _mm512_set1_pd(B[4]);
            C4[0] = _mm512_fmadd_pd(a0, b4, C4[0]);
            C4[1] = _mm512_fmadd_pd(a1, b4, C4[1]);
            C4[2] = _mm512_fmadd_pd(a2, b4, C4[2]);
        }
        if (nr > 5) {
            __m512d b5 = _mm512_set1_pd(B[5]);
            C5[0] = _mm512_fmadd_pd(a0, b5, C5[0]);
            C5[1] = _mm512_fmadd_pd(a1, b5, C5[1]);
            C5[2] = _mm512_fmadd_pd(a2, b5, C5[2]);
        }

        A += MR;
        B += NR;
    }

    // 3) store masked
    #define STORE_C_M(col,Cv)                                \
      _mm512_mask_storeu_pd(C + (col)*ldC +  0, m0, Cv[0]);  \
      _mm512_mask_storeu_pd(C + (col)*ldC +  8, m1, Cv[1]);  \
      _mm512_mask_storeu_pd(C + (col)*ldC + 16, m2, Cv[2])
    switch(nr) {
      default: STORE_C_M(5, C5);
      case 5:  STORE_C_M(4, C4);
      case 4:  STORE_C_M(3, C3);
      case 3:  STORE_C_M(2, C2);
      case 2:  STORE_C_M(1, C1);
      case 1:  STORE_C_M(0, C0);
      case 0: ;
    }
    #undef STORE_C_M
}
